---
title: "Assignment 2"
---
General considerations website layout:  
Data about the bribe; time, place, amount, category, etc. is available from the main search page.  
The full text describing the event is only available from the link itself. 
Can get most of the data from the main "search", without having to go through every link that has been pulled  


Let's generate and scrape the links of the reports first
```{r}
install.packages("rvest")
install.packages("dplyr")
install.packages("xml2")
library("rvest")
library("dplyr")
library("xml2")

# Create Dataset

library("rvest")
library("dplyr")
library("xml2")
library("readr")
library("stringr")
library("ggplot2")

## Initialize dataframe with the desired data columns
df <- data_frame(title = character(), 
                 amount = character(), 
                 dep = character(), 
                 trans = character(), 
                 views = character(),
                 city = character(),
                 date = character(), 
                 time = character())

## Create empty dataframe
dftemp <- data_frame()

## Scrape data from web

for (i in 0:99) { #loop through first 100 pages, 10 results per page = 1000 
  link <- paste("http://www.ipaidabribe.com/reports/paid?page=",i*10, sep = "") #Create hyperlink based on loop function
  print(paste("processing", i, sep = " ")) #progress report
  main <- read_html(link, encoding = "UTF-8") #define the static part of link references
  
  title <- main %>% 
    html_nodes(".heading-3 a") %>% 
    html_text() 
  
  amount <- main %>% # feed `main.page` to the next step
    html_nodes(".paid-amount span") %>% # get the CSS nodes
    html_text() # extract the link text
  
  dep <- main %>% # feed `main.page` to the next step
    html_nodes(".name a") %>% # get the CSS nodes
    html_text() # extract the link text
  
  trans <- main %>% # feed `main.page` to the next step
    html_nodes(".transaction a") %>% # get the CSS nodes
    html_text() # extract the link text
  
  views <- main %>% # feed `main.page` to the next step
    html_nodes(".overview .views") %>% # get the CSS nodes
    html_text() # extract the link text
  
  city <- main %>% # feed `main.page` to the next step
    html_nodes(".location") %>% # get the CSS nodes
    html_text() # extract the link text
  
  date <- main %>% 
    html_nodes(".date") %>% 
    html_text() 
  
  time <- main %>% 
    html_nodes(".time-span") %>% 
    html_text() 
  
  dftemp <- cbind(title, amount, dep, trans, views, city, date, time) #bind the variables together into a 10 by n dataframe
  df <- rbind(df,dftemp)
  
  Sys.sleep(1) #timer, wait 1 second
  cat(" done!\n") #progress report
}

#clean unused variables from workspace
rm("title", "amount", "dep", "trans", "views", "city", "date", "time", "dftemp", "i")

## Split the city column

df$states <- lapply(strsplit(as.character(df$city), "\\,"), "[", 2)
df$city <- lapply(strsplit(as.character(df$city), "\\,"), "[", 1)

## Clean and order dataset

library(stringr)

df$title <- df$title %>% #clean text
  str_replace_all(pattern = "\\n" , replacement = " ") %>%
  str_trim()

df$amount <- df$amount %>% #clean text from amount and convert to numeric
  str_replace_all(pattern = "Paid INR" , replacement = " ") %>% 
  str_replace_all(pattern = "," , replacement = "") %>% 
  str_trim() %>% 
  as.numeric() 

df$views <- df$views %>% #clean text from views and convert to numeric
  str_replace_all(pattern = "views" , replacement = " ") %>%
  str_trim() %>% 
  as.numeric()

df$city <- df$city %>% #clean text from city
  as.character(df$city) %>%
  str_trim()

df$states <- df$states %>% #clean text from states
  as.character(df$states) %>%
  str_trim()

df$time <- as.numeric(str_extract(df$time,"[0-9]*"))*!grepl("minutes|hours",df$time) #clean hours and minutes out of time stamp and change to whole number of days

df$date <- as.Date(df$date, format("%B %d, %Y")) #convert the date column to date format

df <- df[, c(1,2,3,4,5,6,9,7,8)]

## Suggestion: Analysis of Variance between transaction and amount per province or city

# Alternative to start of : 
# Arbirtrary list of variables and header locations in link, variable observations per pages
# 
# headers <- c("title", "amount", "dep", "trans", "views", "city", "date", "time", "link") #list of column names
# nodes <- c(".heading-3 a",".paid-amount span", ".name a", ".transaction a", ".overview .views", ".location", ".date", #".time-span", ".heading-3 a") #corresponding css nodes
# obsnr <- 10 #number of observations per page
# pgnr <- 99 #number of pages to loop through +1 (starts at 0)
# df <- data_frame() #initialize df as data frame
# dftemps <- data_frame() #initialize dftemps as dataframe
# 
# for (i in 0:pgnr) { #loop through first 100 pages, 10 results per page = 1000 
#   link <- paste("http://www.ipaidabribe.com/reports/paid?page=",i*obsnr, sep = "") #Create hyperlink based on loop function
#   print(paste("processing", i, sep = " ")) #progress report outer loop
#   main <- read_html(link, encoding = "UTF-8") #define the static part of link references
#       #begin inner loop, defining the dftemps results based on the header and node vectors
#       for (k in 1:length(headers)) { #dynamic length based on the number of headers
#       print(paste("processing", headers[[k]], sep = " ")) #progress report inner loop
#       dftemps[1:obsnr, headers[[k]]] <- #define the column name by the headers vector
#       if(headers[[k]] != "link") #test if the data is the link data, if it's not, standard html_text, if it is, html_attr()
#         main %>% #create column based on header name, fill with node data
#         html_nodes(nodes[[k]]) %>% 
#         html_text()
#       else #this is if it is the link data
#         main %>% 
#         html_nodes(nodes[[k]]) %>% 
#         html_attr(name = 'href')
#       }
#   
#   df <- rbind(df,dftemps) #bind dftemps to df, and iterate to next link.
#   Sys.sleep(1) #timer, wait 1 second
#   cat(" done!\n") #progress report
#   
#   }  
 


#Import the common csv file from github to work on

df <- read_csv("https://raw.githubusercontent.com/TorJensen/Assignment-2/master/df.csv")
df <- df[,2:10] #remove the rownames column
df <- filter(df, dep != "") #Remove entry with missing data


#sort data and have a look at the top 10 largest amounts to get an idea of outliers
df %>% 
  arrange(desc(amount)) %>% 
  head(20) 
#one single value that is incredibly high, not necesarily incorrect but maybe clean it out for averages?

#sorted in ascending order
df %>% 
  arrange(desc(desc(amount))) %>% 
  head(20)
#quite a few instances of Rs <= 10 (equivalent to about 14euro cent), maybe not so relevant, but not too crazy 


#basic plot showing amount and views 
p <- ggplot(df,aes(views, amount))
p + geom_point()

#is clearly an issue with outlier, clean it out: #this commented out for now
#df <- df %>% 
#  filter(amount < max(amount))

#try again, filtering out largest value and taking logs:
#basic plot showing amount and views, logs of aount to deal with higher numbers
p <- ggplot(df %>% filter(amount < max(amount)),aes(views, log(amount)))
p + geom_point(alpha = 0.4)
#doesn't seem to be any real relation, few outliers are for medium-sized amounts, but outlier status seems to be driven by other things than size in general

#Number of bribes and their sizes, by variables in various combinations: 

#department
df_dep <- df %>% 
  group_by(dep) %>% 
  summarise(count = n(), sum = sum(amount))%>% 
  mutate(avgamount = round(sum/count))

#state
df_state <- df %>% 
  group_by(states) %>% 
  summarise(count = n(), sum = sum(amount))%>% 
  mutate(avgamount = round(sum/count))

#department and transaction type
df_deptrans <- df %>% 
  group_by(dep,trans) %>% 
  summarise(count = n(), sum = sum(amount))%>% 
  mutate(avgamount = round(sum/count))

#state and city
df_statecity <- df %>% 
  group_by(states, city) %>% 
  summarise(count = n(), sum = sum(amount)) %>% 
  mutate(avgamount = round(sum/count))

#state and department, the thougth was that some departments could be more corrupt in some areas compared to others
df_statedep <- df %>% 
  group_by(states, dep) %>% 
  summarise(count = n(), sum = sum(amount)) %>% 
  mutate(avgamount = round(sum/count))

#number of views by state, are some states viewed more frequently, do states with lots of reports have more views, do states with a higher value of bribes have more views
df_stateviews <- df %>% 
  group_by(states) %>% 
  summarise(count = n(), views = sum(views), amount = sum(amount)) %>% 
  mutate(avgviews = round(views/count), avgamount = round(amount/count))

#number of views by department, are some departments viewed more frequently, do departments with lots of reports have more views, do departments with higher bribe value have more views.  
df_depviews <- df %>% 
  group_by(dep) %>% 
  summarise(count = n(), views = sum(views), amount = sum(amount)) %>% 
  mutate(avgviews = round(views/count), avgamount = round(amount/count))

#does number of views level out with time? 
df_viewsdate <- df %>% 
  group_by(date) %>% 
  summarise(count = n(), views = sum(views)) %>% 
  mutate(avgviews = round(views/count))
#maybe a little bit in general, but interesting thing comes up, more than 600 observations were uploaded on the same day last month, none of these were viewed very much at all, could be worth looking into why this was - robots? press coverage? specific pattern? 

#plot showing it
p <- ggplot(df_viewsdate,aes(date, count))
p + geom_bar(stat = "identity")

# commented this out while playing with dplyr
# bribe_sta <- aggregate(amount ~ states, df, sum)
# bribe_sta <- bribe_sta[-c(1),]
# bribe_sta <- bribe_sta %>% arrange(desc(amount))
